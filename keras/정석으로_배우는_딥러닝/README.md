# **정석으로 배우는 딥러닝** : 텐서플로와 케라스로 배우는 시계열 데이터 처리 알고리즘

<img src='http://developerfarm.cdn1.cafe24.com/cover/s/9791158390822.jpg'>

---

## 목차

#### 1. 수학 지식 준비
- 1.1 편미분
  - 1.1.1 도함수와 편도함수
  - 1.1.2 미분 계수롸 편미분 계수
  - 1.1.3 편미분의 기본 공식
  - 1.1.4 합성함수의 편미분
  - 1.1.5 레벨 업 전미분
- 1.2 선형대수
  - 1.2.1 벡터
  - 1.2.2 행렬
- 1.3 정리

#### 2. 파이썬 준비
- 2.1 파이썬 2와 파이썬 3
- 2.2 아나콘다 배포판
- 2.3 파이썬 기초
  - 2.3.1 파이썬 프로그램 실행
  - 2.3.2 데이터형
  - 2.3.3 변수
  - 2.3.4 데이터 구조
  - 2.3.5 연산
  - 2.3.6 기본 구문
  - 2.3.7 함수
  - 2.3.8 클래스
  - 2.3.9 라이브러리
- 2.4 NumPy
  - 2.4.1 NumPy 배열
  - 2.4.2 NumPy로 벡터, 행렬 계산
  - 2.4.3 배열과 다차원 배열 생성
  - 2.4.4 슬라이스
  - 2.4.5 브로드캐스트
- 2.5 딥러닝을 위한 라이브러리
  - 2.5.1 TensorFlow
  - 2.5.2 Keras
  - 2.5.3 Theano
- 2.6 정리

#### 3. 신경망
- 3.1 신경망이란?
  - 3.1.1 뇌와 신경망
  - 3.1.2 딥러닝과 신경망
- 3.2 신경망이라는 회로
  - 3.2.1 단순한 모델화
  - 3.2.2 논리 회로
- 3.3 단순 퍼셉트론
  - 3.3.1 모델화
  - 3.3.2 구현
- 3.4 로지스틱 회귀
  - 3.4.1 계단함수와 시그모이드 함수
  - 3.4.2 모델화
  - 3.4.3 구현
  - 3.4.4 (레벨업) 시그모이드 함수와 확률밀도함수, 누적분포함수
  - 3.4.5 (레벨업) 경사하강법과 국소최적
- 3.5 다중 클래스 로지스틱 회귀
  - 3.5.1 소프트맥스 함수
  - 3.5.2 모델화
  - 3.5.3 구현
- 3.6 다층 퍼셉트론
  - 3.6.1 비선형 분류
  - 모델화
  - 구현
- 3.7 모델 평가
  - 3.7.1 분류에서 예측으로
  - 3.7.2 예측을 평가
  - 3.7.3 간단한 실험
- 3.8 정리

#### 4. 심층 신경망
- 4.1 딥러닝 준비
- 4.2 학습시킬 때 발생하는 문제
  - 4.2.1 Vanishing Gradient 문제
  - 4.2.2 Overfitting 문제
- 4.3 효율적인 학습을 위해
  - 4.3.1 Activation Function
  - 4.3.2 Dropout
- 4.4 구현 설계
  - 4.4.1 기본 설계
  - 4.4.2 Visualizing
- 4.5 고급 기술
  - 4.5.1 Data Normalization & Initialization
  - 4.5.2 Learning Rate
  - 4.5.3 Early Stopping
  - 4.5.4 Batch Normalization
- 4.6 정리

#### 5. 순환 신경망
- 5.1 기본 사항
  - 5.1.1 시계열 데이터
  - 5.1.2 과거의 Hidden layer
  - 5.1.3 Backpropagation Through Time
  - 5.1.4 구현
- 5.2 LSTM
  - 5.2.1 LSTM 블록
  - 5.2.2 CEC, Input, Output Gate
  - 5.2.3 Forget Gate
  - 5.2.4 핍홀 결합
  - 5.2.5 모델화
  - 5.2.6 구현
  - 5.2 7 장기 의존성 학습 평가 - Adding Problem
- 5.3 GRU
  - 5.3.1 모델화
  - 5.3.2 구현
- 5.4 정리

#### 6. 순환 신경망 응용
- 6.1 Bidirectional RNN
  - 6.1.1 미래의 은닉층
  - 6.1.2 전방향·후방향 전파
  - 6.1.3 MNIST를 사용한 예측
- 6.2 RNN Encoder-Decoder
  - 6.2.1 Sequence-to-Sequence 모델
  - 6.2.2 간단한 Q&A 문제
- 6.3 Attention
  - 6.3.1 시간의 웨이트
  - 6.3.2 LSTM에서의 Attention
- 6.4 Memory Networks
  - 6.4.1 기억의 외부화
  - 6.4.2 Q&A 문제에 적용
  - 6.4.3 구현
- 6.5 정리

#### 부록
- A.1 모델을 저장하고 읽어 들인다
  - A.1.1 텐서플로에서의 처리
  - A.1.2 케라스에서의 처리
- A.2 텐서보드(TensorBoard)
- A.3 tf.contrib.learn
