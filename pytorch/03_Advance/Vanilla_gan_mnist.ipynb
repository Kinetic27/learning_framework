{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import utils\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='../data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "print(\"Downloading Train Data Done ! \")\n",
    "\n",
    "mnist_test = datasets.MNIST(root='../data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "print(\"Downloading Test Data Done ! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1 = nn.Linear(100, 256)\n",
    "        self.bnorm1 = nn.BatchNorm1d(256)\n",
    "        self.linear2 = nn.Linear(256, 512)\n",
    "        self.bnorm2 = nn.BatchNorm1d(512)\n",
    "        self.linear3 = nn.Linear(512, 784)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.linear1(X)\n",
    "        #X = self.bnorm1(X)\n",
    "        X = F.leaky_relu(X, negative_slope=0.03)\n",
    "        X = self.linear2(X)\n",
    "        #X = self.bnorm2(X)\n",
    "        X = F.leaky_relu(X, negative_slope=0.03)\n",
    "        X = self.linear3(X)\n",
    "        X = torch.sigmoid(X)\n",
    "        return X\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 256)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.linear3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = F.leaky_relu(self.linear1(X), negative_slope=0.03)\n",
    "        X = F.leaky_relu(self.linear2(X), negative_slope=0.03)\n",
    "        X = torch.sigmoid(self.linear3(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "data_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iteration maker Done !\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    avg_loss = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "    for i, (batch_img, _) in enumerate(data_iter):\n",
    "        \n",
    "        X = batch_img.view(batch_size, -1)\n",
    "        \n",
    "        real_lab = torch.ones(batch_size, 1)\n",
    "        fake_lab = torch.zeros(batch_size, 1)\n",
    "        \n",
    "        # Training Discriminator\n",
    "        D_pred = D.forward(X)\n",
    "        d_loss_real = criterion(D_pred, real_lab)\n",
    "        real_score = D_pred\n",
    "        \n",
    "        z = torch.randn(batch_size, 100)\n",
    "        fake_images = G.forward(z)\n",
    "        G_pred = D.forward(fake_images)\n",
    "        d_loss_fake = criterion(G_pred, fake_lab)\n",
    "        fake_score = G_pred\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Training Generator\n",
    "        z = torch.randn(batch_size, 100)\n",
    "        fake_images = G.forward(z)\n",
    "        G_pred = D.forward(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(G_pred, real_lab)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1)%100 == 0 :\n",
    "            print(\"Epoch : \", epoch+1, \"Iteration : \", i+1)\n",
    "    print(\"Epoch : \", epoch+1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
